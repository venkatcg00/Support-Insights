services:
  mysql:
    image: mysql:latest
    container_name: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: ${PROJECT_PASSWORD}
      MYSQL_DATABASE: ${DB_NAME}
      MYSQL_USER: ${PROJECT_USER}
      MYSQL_PASSWORD: ${PROJECT_PASSWORD}
    ports:
      - "${DB_PORT}:${DB_PORT}"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./mysql-init:/docker-entrypoint-initdb.d
    networks:
      - data-network
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "${PROJECT_USER}", "-p${PROJECT_PASSWORD}"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 10s

  mongodb:
    image: mongo:latest
    container_name: mongodb
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${PROJECT_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${PROJECT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_DB}
    ports:
      - "${MONGO_PORT}:${MONGO_PORT}"
    volumes:
      - mongo_data:/data/db
    networks:
      - data-network

  zookeeper:
    image: bitnami/zookeeper:latest
    container_name: zookeeper
    restart: always
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/bitnami/zookeeper
    networks:
      - data-network

  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    restart: always
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${KAFKA_BOOTSTRAP_SERVERS},PLAINTEXT_HOST://localhost:29092
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - ALLOW_PLAINTEXT_LISTENER=yes
    volumes:
      - kafka_data:/bitnami/kafka
    networks:
      - data-network

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: always
    depends_on:
      - kafka
    ports:
      - "1234:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
    networks:
      - data-network

  minio:
    image: minio/minio
    container_name: minio
    restart: always
    ports:
      - "${MINIO_PORT}:${MINIO_PORT}"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${PROJECT_USER}
      MINIO_ROOT_PASSWORD: ${PROJECT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - data-network

  airflow:
      image: apache/airflow:latest
      container_name: airflow
      restart: always
      ports:
        - "8080:8080"
      environment:
        AIRFLOW__CORE__EXECUTOR: LocalExecutor
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://${PROJECT_USER}:${PROJECT_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}
        AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: 30
        AIRFLOW__CORE__DAG_SERIALIZATION: "True"
        AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
        AIRFLOW__CORE__LOAD_EXAMPLES: "False"
        AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
        _AIRFLOW_WWW_USER_USERNAME: ${PROJECT_USER}
        _AIRFLOW_WWW_USER_PASSWORD: ${PROJECT_PASSWORD}
        _AIRFLOW_WWW_USER_FIRSTNAME: ${AIRFLOW_FIRSTNAME}
        _AIRFLOW_WWW_USER_LASTNAME: ${AIRFLOW_LASTNAME}
        _AIRFLOW_WWW_USER_EMAIL: ${AIRFLOW_EMAIL}
        _AIRFLOW_WWW_USER_ROLE: Admin
        DB_HOST: ${DB_HOST}
        DB_PORT: ${DB_PORT}
        DB_NAME: ${DB_NAME}
        DB_USER: ${PROJECT_USER}
        DB_PASSWORD: ${PROJECT_PASSWORD}
        MONGO_HOST: ${MONGO_HOST}
        MONGO_PORT: ${MONGO_PORT}
        MONGO_USER: ${PROJECT_USER}
        MONGO_PASSWORD: ${PROJECT_PASSWORD}
        MONGO_DB: ${MONGO_DB}
        MINIO_HOST: ${MINIO_HOST}
        MINIO_PORT: ${MINIO_PORT}
        MINIO_USER: ${PROJECT_USER}
        MINIO_PASSWORD: ${PROJECT_PASSWORD}
        MINIO_BUCKET: ${MINIO_BUCKET}
        KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
        KAFKA_TOPIC_NAME: ${KAFKA_TOPIC_NAME}
      volumes:
        - ./Data-Loaders/Batch-Processing:/opt/airflow/dags
        - ./Data-Loaders/plugins:/opt/airflow/plugins
        - airflow_logs:/opt/airflow/logs  # Named volume for Airflow logs
      networks:
        - data-network
      depends_on:
        mysql:
          condition: service_healthy
      command: >
        bash -c "
        echo 'Initializing Airflow...' &&
        airflow db init &&
        pip install --no-cache-dir boto3 pandas kafka-python pymysql pymongo confluent-kafka pyspark --break-system-packages &&
        airflow users create -u ${PROJECT_USER} -p ${PROJECT_PASSWORD} -f ${AIRFLOW_FIRSTNAME} -l ${AIRFLOW_LASTNAME} -r Admin -e ${AIRFLOW_EMAIL} &&
        echo 'Airflow DB initialized and user created.' &&
        airflow webserver & airflow scheduler
        "


  spark:
    image: bitnami/spark:latest
    container_name: spark
    restart: always
    ports:
      - "${SPARK_MASTER_WEBUI_PORT}:${SPARK_MASTER_WEBUI_PORT}"
      - "${SPARK_MASTER_PORT}:${SPARK_MASTER_PORT}"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=${SPARK_MASTER_HOST}
      - SPARK_MASTER_PORT=${SPARK_MASTER_PORT}
      - SPARK_MASTER_WEBUI_PORT=${SPARK_MASTER_WEBUI_PORT}
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_NAME=${DB_NAME}
      - DB_USER=${PROJECT_USER}
      - DB_PASSWORD=${PROJECT_PASSWORD}
      - MONGO_HOST=${MONGO_HOST}
      - MONGO_PORT=${MONGO_PORT}
      - MONGO_USER=${PROJECT_USER}
      - MONGO_PASSWORD=${PROJECT_PASSWORD}
      - MONGO_DB=${MONGO_DB}
      - MINIO_HOST=${MINIO_HOST}
      - MINIO_PORT=${MINIO_PORT}
      - MINIO_USER=${PROJECT_USER}
      - MINIO_PASSWORD=${PROJECT_PASSWORD}
      - MINIO_BUCKET=${MINIO_BUCKET}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_TOPIC_NAME=${KAFKA_TOPIC_NAME}
    volumes:
      - ./Data-Loaders/Stream-Processing:/opt/spark/scripts
      - spark_logs:/opt/spark/checkpoints
      - spark_logs:/opt/bitnami/spark/logs 
    networks:
      - data-network
    depends_on:
      mysql:
        condition: service_healthy
      kafka:
        condition: service_started
    command: >
      bash -c "
      pip install --no-cache-dir sqlalchemy pymysql kafka-python pymongo boto3 --break-system-packages &&
      until bash -c 'exec 3<>/dev/tcp/mysql/3306 && echo MySQL is up'; do echo 'Waiting for MySQL...'; sleep 5; done &&
      until bash -c 'exec 3<>/dev/tcp/kafka/9092 && echo Kafka is up'; do echo 'Waiting for Kafka...'; sleep 5; done &&
      /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} & 
      /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master
      "

  python-runner:
    image: python:3.9-slim
    container_name: python-runner
    restart: always
    ports:
      - "1212:1212"
    environment:
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_NAME=${DB_NAME}
      - DB_USER=${PROJECT_USER}
      - DB_PASSWORD=${PROJECT_PASSWORD}
      - MONGO_HOST=${MONGO_HOST}
      - MONGO_PORT=${MONGO_PORT}
      - MONGO_USER=${PROJECT_USER}
      - MONGO_PASSWORD=${PROJECT_PASSWORD}
      - MONGO_DB=${MONGO_DB}
      - MINIO_HOST=${MINIO_HOST}
      - MINIO_PORT=${MINIO_PORT}
      - MINIO_USER=${PROJECT_USER}
      - MINIO_PASSWORD=${PROJECT_PASSWORD}
      - MINIO_BUCKET=${MINIO_BUCKET}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_TOPIC_NAME=${KAFKA_TOPIC_NAME}
    volumes:
      - ./Data-Generators:/app/scripts
      - python_runner_logs:/app/logs  # Named volume for python-runner logs
    networks:
      - data-network
    depends_on:
      mysql:
        condition: service_healthy
    command: >
      bash -c "
      apt-get update && apt-get install -y netcat-openbsd && apt-get clean && rm -rf /var/lib/apt/lists/* &&
      pip install --no-cache-dir sqlalchemy pymysql kafka-python pymongo boto3 --break-system-packages &&
      tail -f /dev/null
      "

networks:
  data-network:
    name: data-network

volumes:
  mysql_data:
  mongo_data:
  minio_data:
  kafka_data:
  zookeeper_data:
  airflow_logs:      # Named volume for Airflow logs
  spark_logs:        # Named volume for Spark logs
  python_runner_logs: # Named volume for python-runner logs